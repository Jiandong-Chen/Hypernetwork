import torch
import torch.nn as nn
import torch.nn.functional as F

class HyperNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(HyperNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        # self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        # self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        # x = self.bn1(x)
        x = torch.tanh(self.fc2(x))
        # x = self.bn2(x)
        return self.fc3(x)
    
# class ClassicMLP(nn.Module):
#     def __init__(self, input_dim, hidden_dim, output_dim):
#         super(ClassicMLP, self).__init__()
#         self.fc1 = nn.Linear(input_dim, hidden_dim)
#         self.bn1 = nn.BatchNorm1d(hidden_dim)  # BatchNorm layer after fc1
#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)
#         self.bn2 = nn.BatchNorm1d(hidden_dim)  # BatchNorm layer after fc2
#         self.fc3 = nn.Linear(hidden_dim, output_dim)
    
#     def forward(self, x):
#         x = F.relu(self.fc1(x))
#         x = self.bn1(x)  # Apply BatchNorm
#         x = F.tanh(self.fc2(x))
#         x = self.bn2(x)  # Apply BatchNorm
#         return self.fc3(x)
    
class MainMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MainMLP, self).__init__()
        self.hidden_dim = hidden_dim
        
        # Hypernetwork to generate weights
        self.hyper_w1 = HyperNetwork(input_dim, hidden_dim, input_dim * hidden_dim)
        self.hyper_w2 = HyperNetwork(input_dim, hidden_dim, hidden_dim * hidden_dim)
        self.hyper_w3 = HyperNetwork(input_dim, hidden_dim, hidden_dim * output_dim)
        
        # Bias terms (not generated by hypernetwork)
        self.b1 = nn.Parameter(torch.zeros(hidden_dim))
        self.b2 = nn.Parameter(torch.zeros(hidden_dim))
        self.b3 = nn.Parameter(torch.zeros(output_dim))

        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
    
    def forward(self, x):
        batch_size = x.size(0)

        W1 = self.hyper_w1(x).view(batch_size, self.hidden_dim, -1)
        W2 = self.hyper_w2(x).view(batch_size, self.hidden_dim, self.hidden_dim)
        W3 = self.hyper_w3(x).view(batch_size, -1, self.hidden_dim)
        
        # Forward pass using dynamically generated weights
        h1 = torch.bmm(W1, x.unsqueeze(-1)).squeeze(-1) + self.b1
        h1 = 1/4 * F.relu(h1)
        h1 = self.bn1(h1)
        
        h2 = torch.bmm(W2, h1.unsqueeze(-1)).squeeze(-1) + self.b2
        h2 = 1/4 * F.relu(h2)
        h2 = self.bn2(h2)
        
        out = torch.bmm(W3, h2.unsqueeze(-1)).squeeze(-1) + self.b3
        
        return out
    
class MainMLP_random_z_version1(nn.Module):
    def __init__(self, input_dim, seed_length, hidden_dim, output_dim):
        super(MainMLP_random_z_version1, self).__init__()
        self.hidden_dim = hidden_dim
        self.seed_length = seed_length
        
        # Hypernetwork to generate weights
        self.hyper_w1 = HyperNetwork(self.seed_length, hidden_dim, input_dim * hidden_dim)
        self.hyper_w2 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * hidden_dim)
        self.hyper_w3 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * output_dim)
        
        # Bias terms (not generated by hypernetwork)
        self.b1 = nn.Parameter(torch.zeros(hidden_dim))
        self.b2 = nn.Parameter(torch.zeros(hidden_dim))
        self.b3 = nn.Parameter(torch.zeros(output_dim))

        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)

        self.W1 = None
        self.W2 = None
        self.W3 = None
        
    def forward(self, x):

        if self.training:
        
            w_init = torch.randn(1, self.seed_length, device=x.device)

            self.W1 = self.hyper_w1(w_init).view(self.hidden_dim, -1)  # Remove batch_size dependence
            self.W2 = self.hyper_w2(w_init).view(self.hidden_dim, self.hidden_dim)
            self.W3 = self.hyper_w3(w_init).view(-1, self.hidden_dim)
            
            # Forward pass using dynamically generated weights
            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        else:


            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        return out
    
class MainMLP_random_z_version2(nn.Module):
    def __init__(self, input_dim, seed_length, hidden_dim, output_dim):
        super(MainMLP_random_z_version2, self).__init__()
        self.hidden_dim = hidden_dim
        self.seed_length = seed_length
        
        # Hypernetwork to generate weights
        self.hyper_w1 = HyperNetwork(self.seed_length, hidden_dim, input_dim * hidden_dim)
        self.hyper_w2 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * hidden_dim)
        self.hyper_w3 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * output_dim)
        
        # Bias terms (not generated by hypernetwork)
        self.b1 = nn.Parameter(torch.zeros(hidden_dim))
        self.b2 = nn.Parameter(torch.zeros(hidden_dim))
        self.b3 = nn.Parameter(torch.zeros(output_dim))

        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)

        self.W1 = None
        self.W2 = None
        self.W3 = None

        self.w_init = torch.randn(1, self.seed_length)
        
    def forward(self, x):

        self.w_init = self.w_init.to(x.device)

        if self.training:

            self.W1 = self.hyper_w1(self.w_init).view(self.hidden_dim, -1)  # Remove batch_size dependence
            self.W2 = self.hyper_w2(self.w_init).view(self.hidden_dim, self.hidden_dim)
            self.W3 = self.hyper_w3(self.w_init).view(-1, self.hidden_dim)
            
            # Forward pass using dynamically generated weights
            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        else:

            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        return out
    
class MainMLP_random_z_version3(nn.Module):
    def __init__(self, input_dim, seed_length, hidden_dim, output_dim):
        super(MainMLP_random_z_version3, self).__init__()
        self.hidden_dim = hidden_dim
        self.seed_length = seed_length
        
        # Hypernetwork to generate weights
        self.hyper_w1 = HyperNetwork(self.seed_length, hidden_dim, input_dim * hidden_dim)
        self.hyper_w2 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * hidden_dim)
        self.hyper_w3 = HyperNetwork(self.seed_length, hidden_dim, hidden_dim * output_dim)
        
        # Bias terms (not generated by hypernetwork)
        self.b1 = nn.Parameter(torch.zeros(hidden_dim))
        self.b2 = nn.Parameter(torch.zeros(hidden_dim))
        self.b3 = nn.Parameter(torch.zeros(output_dim))

        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)

        self.W1 = None
        self.W2 = None
        self.W3 = None

        self.w_init = torch.randn(1, self.seed_length)
        
    def forward(self, x):

        w_init = 0.9 * self.w_init + 0.1 * torch.randn(1, self.seed_length)
        
        w_init = w_init.to(x.device)

        if self.training:

            self.W1 = self.hyper_w1(w_init).view(self.hidden_dim, -1)  # Remove batch_size dependence
            self.W2 = self.hyper_w2(w_init).view(self.hidden_dim, self.hidden_dim)
            self.W3 = self.hyper_w3(w_init).view(-1, self.hidden_dim)
            
            # Forward pass using dynamically generated weights
            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        else:

            h1 = torch.matmul(self.W1, x.T).T + self.b1  # No more batch dependency
            h1 = 1/4 * F.relu(h1)
            h1 = self.bn1(h1)

            h2 = torch.matmul(self.W2, h1.T).T + self.b2
            h2 = 1/4 * F.relu(h2)
            h2 = self.bn2(h2)

            out = torch.matmul(self.W3, h2.T).T + self.b3  # Output

        return out

# class MainMLP_random_z(nn.Module):
#     def __init__(self, input_dim, hidden_dim, output_dim):
#         super(MainMLP_random_z, self).__init__()
#         self.hidden_dim = hidden_dim
        
#         # Hypernetwork to generate weights
#         self.hyper_w1 = HyperNetwork(input_dim, hidden_dim, input_dim * hidden_dim)
#         self.hyper_w2 = HyperNetwork(input_dim, hidden_dim, hidden_dim * hidden_dim)
#         self.hyper_w3 = HyperNetwork(input_dim, hidden_dim, hidden_dim * output_dim)
        
#         # Bias terms (not generated by hypernetwork)
#         self.b1 = nn.Parameter(torch.zeros(hidden_dim))
#         self.b2 = nn.Parameter(torch.zeros(hidden_dim))
#         self.b3 = nn.Parameter(torch.zeros(output_dim))

#         self.bn1 = nn.BatchNorm1d(hidden_dim)
#         self.bn2 = nn.BatchNorm1d(hidden_dim)
#         self.w_init_1 = 
        
    
#     def forward(self, x):

#         batch_size = x.size(0)
        
#         w_init = self.w_init_1 + lambda * torch.randn_like(x)

#         W1 = self.hyper_w1(w_init).view(batch_size, self.hidden_dim, -1)
#         W2 = self.hyper_w2(w_init).view(batch_size, self.hidden_dim, self.hidden_dim)
#         W3 = self.hyper_w3(w_init).view(batch_size, -1, self.hidden_dim)
        
#         # Forward pass using dynamically generated weights
#         h1 = torch.bmm(W1, x.unsqueeze(-1)).squeeze(-1) + self.b1
#         h1 = 1/4 * F.relu(h1)
#         h1 = self.bn1(h1)
        
#         h2 = torch.bmm(W2, h1.unsqueeze(-1)).squeeze(-1) + self.b2
#         h2 = 1/4 * F.relu(h2)
#         h2 = self.bn2(h2)
        
#         out = torch.bmm(W3, h2.unsqueeze(-1)).squeeze(-1) + self.b3
        
#         return out
    
    # def save_current_ws(self):


class MainMLP_random_z_check(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MainMLP_random_z_check, self).__init__()
        self.hidden_dim = hidden_dim
        
        # Hypernetwork to generate weights
        self.hyper_w1 = HyperNetwork(input_dim, hidden_dim, input_dim * hidden_dim)
        self.hyper_w2 = HyperNetwork(input_dim, hidden_dim, hidden_dim * hidden_dim)
        self.hyper_w3 = HyperNetwork(input_dim, hidden_dim, hidden_dim * output_dim)
        
        # Bias terms (not generated by hypernetwork)
        self.b1 = nn.Parameter(torch.zeros(hidden_dim))
        self.b2 = nn.Parameter(torch.zeros(hidden_dim))
        self.b3 = nn.Parameter(torch.zeros(output_dim))

        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
    
    def forward(self, x, w_init):
        batch_size = x.size(0)

        W1 = self.hyper_w1(w_init).view(batch_size, self.hidden_dim, -1)
        W2 = self.hyper_w2(w_init).view(batch_size, self.hidden_dim, self.hidden_dim)
        W3 = self.hyper_w3(w_init).view(batch_size, -1, self.hidden_dim)
        
        # Forward pass using dynamically generated weights
        h1 = torch.bmm(W1, x.unsqueeze(-1)).squeeze(-1) + self.b1
        h1 = 1/4 * F.relu(h1)
        h1 = self.bn1(h1)
        
        h2 = torch.bmm(W2, h1.unsqueeze(-1)).squeeze(-1) + self.b2
        h2 = 1/4 * F.relu(h2)
        h2 = self.bn2(h2)
        
        out = torch.bmm(W3, h2.unsqueeze(-1)).squeeze(-1) + self.b3

        return out
    
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim=100, num_layers=10, output_dim=1):
        super().__init__()
        bias = False
        layers = [nn.Linear(input_dim, hidden_dim, bias=bias)]
        for _ in range(num_layers-2):
            layers.append(nn.BatchNorm1d(num_features = hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Linear(hidden_dim, hidden_dim, bias=bias))
        if output_dim == 1:
            layers.append(nn.Linear(hidden_dim, output_dim, bias=bias))
        else:
            layers.append(nn.Linear(hidden_dim, output_dim, bias=bias))
        
        # layers = [nn.Linear(input_dim, 1, bias=bias)]
        # layers.append(nn.Sigmoid())
        
        
        self.net = nn.Sequential(*layers)
    def forward(self, X):
        return self.net(X)
    
# Example Usage
# input_dim = 10
# hidden_dim = 20
# output_dim = 3

# model = MainMLP(input_dim, hidden_dim, output_dim)
# x = torch.randn(5, input_dim)  # Batch of 5 samples
# output = model(x)
# print(output.shape)  # Should be [5, output_dim]